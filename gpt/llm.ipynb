{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# unassign the model and free up GPU RAM\n",
    "def freeup_VRAM():\n",
    "    memory_used_before = torch.cuda.memory_reserved(0) / 1024 ** 3\n",
    "    try:\n",
    "        global model, tokenizer\n",
    "        del model, tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    memory_used_after = torch.cuda.memory_reserved(0) / 1024 ** 3\n",
    "    print(f\"Freed up {memory_used_before - memory_used_after:.1f} GB of VRAM.\")\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    prompt, do_sample=True, max_new_tokens=50, temperature=0.9, top_k=50, top_p=0.95\n",
    "):\n",
    "    encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=encoded_input[\"input_ids\"].cuda(),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "model_names = [\n",
    "    \"EleutherAI/gpt-neo-125M\",\n",
    "    \"EleutherAI/gpt-neo-1.3B\",\n",
    "    \"EleutherAI/gpt-neo-2.7B\",\n",
    "    \"EleutherAI/gpt-j-6B\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freed up 2.9 GB of VRAM.\n",
      "Loaded model EleutherAI/gpt-j-6B in int8\n"
     ]
    }
   ],
   "source": [
    "model_idx = 3\n",
    "load_in_8bit = True\n",
    "half_precision = False\n",
    "\n",
    "freeup_VRAM()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_names[model_idx])\n",
    "if load_in_8bit:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_names[model_idx], device_map=\"auto\", load_in_8bit=True\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_names[model_idx],\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if half_precision else \"auto\",\n",
    "    )\n",
    "print(\n",
    "    f\"Loaded model {model_names[model_idx]} in {'int8' if load_in_8bit else 'fp16' if half_precision else 'fp32'}\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# set pad_token_id to eos_token_id because GPT2 does not have a EOS token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.generation_config.pad_token_id = model.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a python code that plots the function y=sin(x) using gnuplot, where x ranges from 0 to 2*pi\n",
      "\n",
      "I have been trying to plot this function with gnuplot, but I have no idea how to plot x between 0 and 2*pi. Can anyone tell me how\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write a python code that plots the function y=sin(x)\"\"\"\n",
    "# prompt = '''飞船登机口处，一群人在那儿拼命地挥手，船上的人也在欢呼，只是每个人的目光都是那么的恋恋不舍。这是第五批运向诺顿星开拓者，仙女星系的三个星球是人类移民的天堂，那里经过三百年的建设已经形成了完备高级的生活设施，而且各方面的福利，教育，以及军事防御都是太阳系标准，甚至更高，但诺顿星……还太原始了些，尤其是还有无孔不入的扎戈族，它们就像地球上的某种古董级生物蟑螂一样的顽强，而且随着人类的进化而进化。这次是诺顿星进行第二期开发计划，愿意去这种地方的多是生活不怎么样的，希望用诺顿星的两年来改善生活，年纪都是在三四十岁甚至更老。可是在人群中却有一个看来相当相当年轻的身影，看起来顶多十四五岁，男孩望着送别区，一个红鼻子老头拼命地挥舞着自己的帽子，老人的眼圈都红了，显然是哭过，是什么理由把自己这么年轻的孩子送上这样的飞船？'''\n",
    "print(generate_text(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please answer the following question:\n",
      "\n",
      "Question: What is the capital of Canada?\n",
      "Answer: Ottawa\n",
      "\n",
      "Question: What is the currency of Switzerland?\n",
      "Answer: Swiss franc\n",
      "\n",
      "Question: Who is the first president of United States?\n",
      "Answer: George Washington\n",
      "\n",
      "Question\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Please answer the following question:\n",
    "\n",
    "Question: What is the capital of Canada?\n",
    "Answer: Ottawa\n",
    "\n",
    "Question: What is the currency of Switzerland?\n",
    "Answer: Swiss franc\n",
    "\n",
    "Question: Who is the first president of United States?\n",
    "Answer:\"\"\"\n",
    "print(generate_text(prompt, max_new_tokens=5, temperature=1, top_p=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  50, 1404]]), 'attention_mask': tensor([[1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"SAT\", return_tensors=\"pt\")\n",
    "print(encoded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    6281 MB |    6281 MB |   42977 MB |   36696 MB |\n",
      "|       from large pool |    6276 MB |    6276 MB |   42954 MB |   36678 MB |\n",
      "|       from small pool |       5 MB |       5 MB |      23 MB |      18 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    6281 MB |    6281 MB |   42977 MB |   36696 MB |\n",
      "|       from large pool |    6276 MB |    6276 MB |   42954 MB |   36678 MB |\n",
      "|       from small pool |       5 MB |       5 MB |      23 MB |      18 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    6362 MB |    6362 MB |   12290 MB |    5928 MB |\n",
      "|       from large pool |    6356 MB |    6356 MB |   12272 MB |    5916 MB |\n",
      "|       from small pool |       6 MB |       6 MB |      18 MB |      12 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   82335 KB |  195836 KB |   15630 MB |   15550 MB |\n",
      "|       from large pool |   81920 KB |  193536 KB |   15579 MB |   15499 MB |\n",
      "|       from small pool |     415 KB |    4099 KB |      51 MB |      51 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     509    |     676    |    3520    |    3011    |\n",
      "|       from large pool |     198    |     226    |    1754    |    1556    |\n",
      "|       from small pool |     311    |     450    |    1766    |    1455    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     509    |     676    |    3520    |    3011    |\n",
      "|       from large pool |     198    |     226    |    1754    |    1556    |\n",
      "|       from small pool |     311    |     450    |    1766    |    1455    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      90    |     105    |     300    |     210    |\n",
      "|       from large pool |      87    |     102    |     291    |     204    |\n",
      "|       from small pool |       3    |       3    |       9    |       6    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       2    |      91    |    1029    |    1027    |\n",
      "|       from large pool |       1    |      25    |     875    |     874    |\n",
      "|       from small pool |       1    |      67    |     154    |     153    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared model VRAM.\n"
     ]
    }
   ],
   "source": [
    "freeup_VRAM()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
