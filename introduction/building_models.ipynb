{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "\n",
      "Just one layer:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0329, -0.0056,  0.0100,  ..., -0.0954, -0.0940, -0.0308],\n",
      "        [ 0.0436, -0.0163, -0.0449,  ...,  0.0976, -0.0716, -0.0569],\n",
      "        [ 0.0178, -0.0235, -0.0987,  ...,  0.0420,  0.0105, -0.0995],\n",
      "        ...,\n",
      "        [-0.0274, -0.0294,  0.0312,  ...,  0.0520,  0.0328, -0.0732],\n",
      "        [ 0.0309,  0.0208, -0.0637,  ..., -0.0384, -0.0449, -0.0659],\n",
      "        [ 0.0268,  0.0843, -0.0537,  ..., -0.0120,  0.0637, -0.0064]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0314,  0.0973, -0.0123, -0.0086, -0.0842,  0.0788, -0.0480,  0.0679,\n",
      "         0.0635, -0.0236,  0.0732,  0.0624,  0.0530, -0.0845, -0.0926, -0.0413,\n",
      "        -0.0346, -0.0785,  0.0558, -0.0461, -0.0237, -0.0518, -0.0396,  0.0049,\n",
      "        -0.0565, -0.0315,  0.0019,  0.0980, -0.0973, -0.0197,  0.0181, -0.0979,\n",
      "        -0.0035,  0.0121, -0.0169,  0.0810,  0.0597,  0.0975,  0.0725,  0.0580,\n",
      "        -0.0427,  0.0203,  0.0017, -0.0967,  0.0670,  0.0357, -0.0790,  0.0308,\n",
      "        -0.0717,  0.0975,  0.0060, -0.0063,  0.0736,  0.0278, -0.0659,  0.0203,\n",
      "         0.0167,  0.0742,  0.0201,  0.0152, -0.0114, -0.0328, -0.0690,  0.0686,\n",
      "        -0.0242, -0.0803,  0.0970,  0.0538,  0.0996,  0.0731,  0.0548, -0.0710,\n",
      "         0.0244,  0.0079, -0.0143,  0.0094, -0.0791,  0.0276, -0.0424,  0.0450,\n",
      "         0.0367, -0.0725, -0.0646, -0.0287,  0.0730,  0.0196,  0.0523, -0.0843,\n",
      "         0.0007, -0.0684, -0.0923, -0.0095,  0.0919,  0.0845, -0.0462,  0.0730,\n",
      "        -0.0632, -0.0184,  0.0194, -0.0568, -0.0314,  0.0624,  0.0047,  0.0469,\n",
      "        -0.0539,  0.0233, -0.0597,  0.0754,  0.0496,  0.0032, -0.0671,  0.0051,\n",
      "        -0.0761,  0.0896,  0.0550, -0.0613,  0.0535,  0.0757,  0.0213, -0.0867,\n",
      "        -0.0315, -0.0882,  0.0732, -0.0998,  0.0365,  0.0139, -0.0625,  0.0365,\n",
      "         0.0461,  0.0051,  0.0013,  0.0481, -0.0477,  0.0471,  0.0550,  0.0810,\n",
      "        -0.0052,  0.0404,  0.0709,  0.0775,  0.0792, -0.0225, -0.0997,  0.0170,\n",
      "         0.0807,  0.0547,  0.0993,  0.0054, -0.0674,  0.0342, -0.0013, -0.0261,\n",
      "         0.0501, -0.0726,  0.0630,  0.0362, -0.0668,  0.0064, -0.0208, -0.0891,\n",
      "        -0.0068, -0.0469,  0.0365, -0.0044, -0.0581,  0.0362,  0.0983, -0.0331,\n",
      "        -0.0658,  0.0543,  0.0623, -0.0411, -0.0149, -0.0914,  0.0320, -0.0947,\n",
      "        -0.0471, -0.0451,  0.0946,  0.0405,  0.0701, -0.0808, -0.0694,  0.0956,\n",
      "        -0.0470,  0.0796, -0.0995,  0.0497,  0.0956, -0.0984, -0.0069, -0.0227,\n",
      "         0.0765,  0.0172,  0.0450,  0.0636,  0.0727, -0.0109,  0.0042, -0.0949],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0654,  0.0615, -0.0549,  ...,  0.0434,  0.0249, -0.0208],\n",
      "        [-0.0515,  0.0549,  0.0585,  ..., -0.0447, -0.0419,  0.0386],\n",
      "        [-0.0508,  0.0531, -0.0546,  ..., -0.0019, -0.0412, -0.0695],\n",
      "        ...,\n",
      "        [ 0.0159, -0.0048, -0.0465,  ..., -0.0008, -0.0070,  0.0449],\n",
      "        [ 0.0637,  0.0047, -0.0131,  ..., -0.0080,  0.0373, -0.0010],\n",
      "        [ 0.0667, -0.0132,  0.0154,  ...,  0.0422,  0.0444,  0.0333]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0501,  0.0141, -0.0635,  0.0466,  0.0656, -0.0018, -0.0072,  0.0510,\n",
      "         0.0313, -0.0629], requires_grad=True)\n",
      "\n",
      "\n",
      "Layer params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0654,  0.0615, -0.0549,  ...,  0.0434,  0.0249, -0.0208],\n",
      "        [-0.0515,  0.0549,  0.0585,  ..., -0.0447, -0.0419,  0.0386],\n",
      "        [-0.0508,  0.0531, -0.0546,  ..., -0.0019, -0.0412, -0.0695],\n",
      "        ...,\n",
      "        [ 0.0159, -0.0048, -0.0465,  ..., -0.0008, -0.0070,  0.0449],\n",
      "        [ 0.0637,  0.0047, -0.0131,  ..., -0.0080,  0.0373, -0.0010],\n",
      "        [ 0.0667, -0.0132,  0.0154,  ...,  0.0422,  0.0444,  0.0333]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0501,  0.0141, -0.0635,  0.0466,  0.0656, -0.0018, -0.0072,  0.0510,\n",
      "         0.0313, -0.0629], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print('The model:')\n",
    "print(tinymodel)\n",
    "\n",
    "print('\\n\\nJust one layer:')\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print('\\n\\nLayer params:')\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.3322, 0.6636, 0.2999]])\n",
      "\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[ 0.1570, -0.1894,  0.5639],\n",
      "        [-0.4279,  0.0428,  0.0191]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2297, 0.0031], requires_grad=True)\n",
      "\n",
      "\n",
      "Output:\n",
      "tensor([[ 0.3253, -0.1049]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print('Input:')\n",
    "print(x)\n",
    "\n",
    "print('\\n\\nWeight and Bias parameters:')\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print('\\n\\nOutput:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1018, 0.4032, 0.0520, 0.7738, 0.1640, 0.3665],\n",
      "         [0.1730, 0.9316, 0.0496, 0.1213, 0.5074, 0.4246],\n",
      "         [0.2593, 0.5818, 0.6720, 0.0519, 0.9631, 0.0138],\n",
      "         [0.9210, 0.6813, 0.4306, 0.1551, 0.2512, 0.9309],\n",
      "         [0.3222, 0.8598, 0.2086, 0.5727, 0.2592, 0.7055],\n",
      "         [0.7572, 0.7808, 0.9047, 0.0415, 0.0035, 0.3969]]])\n",
      "tensor([[[0.9316, 0.9631],\n",
      "         [0.9210, 0.9309]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[13.3854, 10.1611, 13.1639, 13.7074],\n",
      "         [10.4206,  5.9438, 13.8881,  8.5944],\n",
      "         [17.4658, 12.0432, 22.3588, 23.9080],\n",
      "         [17.6546,  7.7525, 16.9287, 22.3771]]])\n",
      "tensor(14.3596)\n",
      "tensor([[[ 0.5485, -1.7160,  0.3929,  0.7746],\n",
      "         [ 0.2454, -1.3041,  1.4455, -0.3867],\n",
      "         [-0.3186, -1.4873,  0.7360,  1.0699],\n",
      "         [ 0.2788, -1.5912,  0.1417,  1.1706]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor(7.4506e-09, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.1545, 1.3975, 0.0000, 0.8493],\n",
      "         [1.6378, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2744, 1.1846, 0.1616, 0.0000],\n",
      "         [1.3006, 0.8008, 1.5168, 1.3135]]])\n",
      "tensor([[[0.0000, 1.3975, 0.5919, 0.8493],\n",
      "         [1.6378, 0.9458, 0.7030, 0.0000],\n",
      "         [0.2744, 1.1846, 0.1616, 0.0000],\n",
      "         [1.3006, 0.8008, 1.5168, 1.3135]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "444d45eb9b2a4a1ac32212bdda4fdf1c1ee4cf6e10ba42f5be1f89dc75e17b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
